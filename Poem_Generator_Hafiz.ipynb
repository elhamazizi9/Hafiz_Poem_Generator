{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation #for our model architecture\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#Natural Language Toolkit for NLP\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "poems = open(\"/hafiz.txt\", \"r\").read() # \"r\" stands for read only file\n",
    "poems = poems.lower() #highly recommended - I didn't do it at first and the outcome was not satisfying\n",
    "poems = poems.translate(str.maketrans(\"\", \"\", punctuation)) #no punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for converting character to int and int to character\n",
    "DT = os.path.basename(\"/hafiz.txt\")\n",
    "words = ''.join(sorted(set(poems)))\n",
    "CI = {c: i for i, c in enumerate(words)}\n",
    "IC = {i: c for i, c in enumerate(words)}\n",
    "\n",
    "pickle.dump(CI, open(f\"{DT}-CI.pickle\", \"wb\"))\n",
    "pickle.dump(IC, open(f\"{DT}-IC.pickle\", \"wb\"))\n",
    "\n",
    "intTxt = np.array([CI[c] for c in poems]) #for working with the data we need first to convert it to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(intTxt)\n",
    "sequences = char_dataset.batch(201, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPL(exp):\n",
    "    data = tf.data.Dataset.from_tensors((exp[:100], exp[100]))\n",
    "    for i in range(1, (len(exp)-1) // 2):\n",
    "        in_ = exp[i: i+100]\n",
    "        out_ = exp[i+100]\n",
    "        other_ds = tf.data.Dataset.from_tensors((in_, out_))\n",
    "        data = data.concatenate(other_ds)\n",
    "    return data\n",
    "\n",
    "dataset = sequences.flat_map(SPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = len(words)\n",
    "\n",
    "def encode(in_, out_):\n",
    "    return tf.one_hot(in_, chars), tf.one_hot(in_, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "38574\n"
     ]
    }
   ],
   "source": [
    "ds = dataset.repeat().shuffle(1024).batch(128, drop_remainder=True)\n",
    "print(len(words))\n",
    "print(len(intTxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 100, 256)          293888    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 826,910\n",
      "Trainable params: 826,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(LSTM(256, input_dim=chars, input_length=100, return_sequences=True)) #not recommended\n",
    "model.add(LSTM(256, input_shape=(100, chars), return_sequences=True)) #100*53 - dim and lenght\n",
    "#make sure setting return_sequences to true, otherwise your model is not capable of producing the layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256))\n",
    "#model.add(LSTM(128))\n",
    "model.add(Dense(chars, activation = \"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 368 steps\n",
      "Epoch 1/32\n",
      "368/368 [==============================] - 9s 24ms/step - loss: 2.8062 - accuracy: 0.2041\n",
      "Epoch 2/32\n",
      "368/368 [==============================] - 7s 19ms/step - loss: 2.2930 - accuracy: 0.3308\n",
      "Epoch 3/32\n",
      "368/368 [==============================] - 7s 20ms/step - loss: 2.0979 - accuracy: 0.3806\n",
      "Epoch 4/32\n",
      "368/368 [==============================] - 7s 20ms/step - loss: 1.9562 - accuracy: 0.4108\n",
      "Epoch 5/32\n",
      "368/368 [==============================] - 7s 20ms/step - loss: 1.8406 - accuracy: 0.4422\n",
      "Epoch 6/32\n",
      "368/368 [==============================] - 7s 20ms/step - loss: 1.6989 - accuracy: 0.4803 0s - loss: 1.714 - ETA: 0s - loss: 1.7035 - accura\n",
      "Epoch 7/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 1.5926 - accuracy: 0.5083\n",
      "Epoch 8/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 1.4953 - accuracy: 0.5382 0s - loss: 1.4992 - accuracy: 0. - ETA: 0s - loss: 1.4971 \n",
      "Epoch 9/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 1.3832 - accuracy: 0.5669\n",
      "Epoch 10/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 1.2524 - accuracy: 0.6085\n",
      "Epoch 11/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 1.1141 - accuracy: 0.6523\n",
      "Epoch 12/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.9964 - accuracy: 0.6909\n",
      "Epoch 13/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.8782 - accuracy: 0.7340\n",
      "Epoch 14/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.7592 - accuracy: 0.7741\n",
      "Epoch 15/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.6378 - accuracy: 0.8142\n",
      "Epoch 16/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.5416 - accuracy: 0.8446 0s - loss: 0.5464 - accuracy:  - ETA: 0s - loss: 0.5450 - ac\n",
      "Epoch 17/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.4672 - accuracy: 0.8701\n",
      "Epoch 18/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.3908 - accuracy: 0.8970\n",
      "Epoch 19/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.3302 - accuracy: 0.9171\n",
      "Epoch 20/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.2766 - accuracy: 0.9319\n",
      "Epoch 21/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.2344 - accuracy: 0.9451\n",
      "Epoch 22/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.2072 - accuracy: 0.9522\n",
      "Epoch 23/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.1752 - accuracy: 0.9626\n",
      "Epoch 24/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.1539 - accuracy: 0.9672\n",
      "Epoch 25/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.1416 - accuracy: 0.9688\n",
      "Epoch 26/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.1291 - accuracy: 0.9727\n",
      "Epoch 27/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.1191 - accuracy: 0.9743\n",
      "Epoch 28/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.1189 - accuracy: 0.9716\n",
      "Epoch 29/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.1022 - accuracy: 0.9780\n",
      "Epoch 30/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.0992 - accuracy: 0.9757\n",
      "Epoch 31/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.0916 - accuracy: 0.9786\n",
      "Epoch 32/32\n",
      "368/368 [==============================] - 8s 21ms/step - loss: 0.0876 - accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "ds = dataset.repeat().shuffle(1024).batch(64, drop_remainder=True) #batch size is 32\n",
    "if not os.path.isdir(\"out\"):\n",
    "    os.mkdir(\"out\")\n",
    "    \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "''''inorder to train your model you need to compile it first\n",
    "    categorical_crossentropy => it is a multiclass problem\n",
    "    try not to use model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])'''\n",
    "\n",
    "    \n",
    "\n",
    "model.fit(ds, steps_per_epoch=(len(intTxt) - 15000) // 64, epochs=32)\n",
    "#since our intTxt lenth is 38574, 15000 results in a good choice for train steps in an epoch\n",
    "\n",
    "model.save(f\"out/{DT}-{100}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Poem generating: 100%|███████████████████████████████████████████████████████████████| 600/600 [00:11<00:00, 50.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "den when dispate\n",
      "if this will not suit such be love small\n",
      "who is the doing that is no mamy framer not ensurrow the skne\n",
      "to alwats out own face\n",
      "hafiz graceful yetts heart and speech of your good\n",
      "on the straight a cul i vist\n",
      "stee and in this curnigi sert\n",
      "the angels in this her hair her heart master will keve a healt will wake\n",
      "the one kempassidg those\n",
      "full of his own path her bidde the skn\n",
      "\n",
      "o mess of divers flowers and in the hand of my lovers sweet lips\n",
      "and hawars that senver though may have seemed depaver\n",
      "and the wind secrets of your face and hair\n",
      "partred my friend\n",
      "brought a charm fragrant bree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CI_ = pickle.load(open(f\"{DT}-CI_.pickle\", \"rb\"))\n",
    "IC_ = pickle.load(open(f\"{DT}-IC_.pickle\", \"rb\"))\n",
    "\n",
    "size = len(CI)\n",
    "model.load_weights(f\"out/{DT}-{100}.h5\")  #loading the file saved\n",
    "\n",
    "pred = \"hafiz shirazi\" #make sure all characters are in lower case if you normalized your dataset to lowercase words\n",
    "produced_poem = \"\" #The poem produced\n",
    "\n",
    "for i in tqdm.tqdm(range(600), \"Poem generating\"): #generates 600 words of poem\n",
    "    X = np.zeros((1, 100, size))\n",
    "    for t, char in enumerate(\"hafiz shirazi\"):\n",
    "        array[0, (100 - len(\"hafiz shirazi\")) + t, CI_[char]] = 1\n",
    "    predicted = model.predict(array, verbose=0)[0]\n",
    "    ind_ = np.argmax(predicted)\n",
    "    char_ = IC_[ind_]\n",
    "    produced_poem += char_ #append it to previous produced poem\n",
    "    pred = pred[1:] + char_\n",
    "\n",
    "\n",
    "print(produced_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
